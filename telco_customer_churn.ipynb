{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OUoPmXxsFht",
        "outputId": "d2cde28e-c630-4d1d-fdee-27df0bbc3fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lime in /usr/local/lib/python3.12/dist-packages (0.2.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from lime) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lime) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.12/dist-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.12/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18->lime) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
            "Loaded data shape: (7043, 33)\n",
            "DataFrame columns: ['CustomerID', 'Count', 'Country', 'State', 'City', 'Zip Code', 'Lat Long', 'Latitude', 'Longitude', 'Gender', 'Senior Citizen', 'Partner', 'Dependents', 'Tenure Months', 'Phone Service', 'Multiple Lines', 'Internet Service', 'Online Security', 'Online Backup', 'Device Protection', 'Tech Support', 'Streaming TV', 'Streaming Movies', 'Contract', 'Paperless Billing', 'Payment Method', 'Monthly Charges', 'Total Charges', 'Churn Label', 'Churn Value', 'Churn Score', 'CLTV', 'Churn Reason']\n",
            "Train/test shapes: (5634, 23) (1409, 23)\n",
            "Baseline AUC: 0.8316, Baseline F1: 0.5820\n",
            "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
            "Best params: {'clf__subsample': 0.6, 'clf__reg_lambda': 10, 'clf__reg_alpha': 0, 'clf__n_estimators': 200, 'clf__max_depth': 3, 'clf__learning_rate': 0.05, 'clf__colsample_bytree': 0.5}\n",
            "Final AUC: 0.8589, Final F1: 0.5965\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.90      0.87      1035\n",
            "           1       0.66      0.55      0.60       374\n",
            "\n",
            "    accuracy                           0.80      1409\n",
            "   macro avg       0.75      0.72      0.73      1409\n",
            "weighted avg       0.80      0.80      0.80      1409\n",
            "\n",
            "Top 5 features by mean(|SHAP|):\n",
            "                        feature  mean_abs_shap\n",
            "0                 contract_term       0.467369\n",
            "1                 Tenure Months       0.388169\n",
            "2  Internet Service_Fiber optic       0.252756\n",
            "3                 Dependents_No       0.235263\n",
            "4                Dependents_Yes       0.193993\n",
            "Selected LIME indices: [1090, 1221, 354, 70, 1010]\n",
            "Outputs saved to folder: churn_outputs\n",
            "Files:\n",
            "- lime_explanation_idx_70.html\n",
            "- lime_explanation_idx_1221.html\n",
            "- local_explanations.json\n",
            "- metrics.json\n",
            "- xgb_churn_model.joblib\n",
            "- lime_explanation_idx_1010.html\n",
            "- shap_feature_importance.csv\n",
            "- local_lime_summary.txt\n",
            "- lime_explanation_idx_1090.html\n",
            "- shap_summary.png\n",
            "- global_shap_summary.txt\n",
            "- lime_explanation_idx_354.html\n",
            "- actionable_strategies.txt\n",
            "\n",
            "=== Summary ===\n",
            "Final AUC: 0.8589126559714795\n",
            "Final F1: 0.5964912280701754\n",
            "\n",
            "Top 5 SHAP features:\n",
            "                     feature  mean_abs_shap\n",
            "               contract_term       0.467369\n",
            "               Tenure Months       0.388169\n",
            "Internet Service_Fiber optic       0.252756\n",
            "               Dependents_No       0.235263\n",
            "              Dependents_Yes       0.193993\n",
            "\n",
            "Selected LIME indices and their pred_proba:\n",
            "- idx 1090: prob 0.922, pred 1, true 1\n",
            "- idx 1221: prob 0.904, pred 1, true 1\n",
            "- idx 354: prob 0.004, pred 0, true 0\n",
            "- idx 70: prob 0.004, pred 0, true 0\n",
            "- idx 1010: prob 0.501, pred 1, true 0\n"
          ]
        }
      ],
      "source": [
        "!pip install lime # Install the missing 'lime' library\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_auc_score, f1_score, classification_report, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "import shap\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import json\n",
        "\n",
        "# -----------------------\n",
        "# Configuration\n",
        "# -----------------------\n",
        "DATA_PATH = \"Telco-Customer-Churn.csv\"  # change if necessary\n",
        "RANDOM_STATE = 42\n",
        "OUTPUT_DIR = \"churn_outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# -----------------------\n",
        "# Load data\n",
        "# -----------------------\n",
        "df = pd.read_csv('Telco_customer_churn.csv')\n",
        "print(\"Loaded data shape:\", df.shape)\n",
        "print(\"DataFrame columns:\", df.columns.tolist()) # Added to inspect column names\n",
        "\n",
        "# -----------------------\n",
        "# Basic cleaning for typical Telco dataset quirks\n",
        "# -----------------------\n",
        "# Some versions store TotalCharges as string with blanks for 0-tenure rows\n",
        "if \"Total Charges\" in df.columns: # Corrected column name\n",
        "    df['Total Charges'] = pd.to_numeric(df['Total Charges'], errors='coerce') # Corrected column name\n",
        "    df['Total Charges'] = df['Total Charges'].fillna(0.0) # Corrected column name\n",
        "\n",
        "# Target encoding\n",
        "if 'Churn Label' in df.columns: # Changed 'Churn' to 'Churn Label'\n",
        "    df['ChurnFlag'] = df['Churn Label'].map({'Yes': 1, 'No': 0}) # Changed 'Churn' to 'Churn Label'\n",
        "    target = 'ChurnFlag'\n",
        "else:\n",
        "    raise ValueError(\"Expecting a column named 'Churn Label' with values 'Yes'/'No'.\") # Changed 'Churn' to 'Churn Label'\n",
        "\n",
        "# -----------------------\n",
        "# Feature Engineering - at least 3 domain-relevant features\n",
        "# -----------------------\n",
        "# 1) num_services: count of paid services (PhoneService, multiple internet-related services)\n",
        "service_cols = [\n",
        "    'Phone Service', 'Multiple Lines', 'Online Security', 'Online Backup',\n",
        "    'Device Protection', 'Tech Support', 'Streaming TV', 'Streaming Movies'\n",
        "]\n",
        "# Ensure columns exist; if not, create placeholders\n",
        "for c in service_cols:\n",
        "    if c not in df.columns:\n",
        "        df[c] = 'No'\n",
        "\n",
        "# binary indicator for internet service presence\n",
        "df['HasInternet'] = df['Internet Service'].apply(lambda x: 0 if str(x).lower() in ['no', 'none', 'nan'] else 1)\n",
        "\n",
        "# Count 'Yes' across service columns (treat 'No phone service' / 'No internet service' as No)\n",
        "def is_yes(x):\n",
        "    if pd.isna(x): return 0\n",
        "    x = str(x).lower()\n",
        "    return 1 if x.startswith('yes') else 0\n",
        "\n",
        "df['num_services'] = df[service_cols].applymap(is_yes).sum(axis=1) + df['HasInternet']\n",
        "\n",
        "# 2) tenure_bin: bucketize tenure\n",
        "def tenure_bucket(t):\n",
        "    if t <= 6: return '0-6'\n",
        "    if t <= 12: return '7-12'\n",
        "    if t <= 24: return '13-24'\n",
        "    if t <= 48: return '25-48'\n",
        "    if t <= 72: return '49-72'\n",
        "    return '72+'\n",
        "df['Tenure Months'] = df['Tenure Months'].astype(int) # Change column 'tenure' to 'Tenure Months'\n",
        "df['tenure_bin'] = df['Tenure Months'].apply(tenure_bucket) # Change column 'tenure' to 'Tenure Months'\n",
        "\n",
        "# 3) avg_charge_per_service: MonthlyCharges / max(1, num_services) to represent value per service\n",
        "df['avg_charge_per_service'] = df.apply(lambda r: r['Monthly Charges'] / max(1, r['num_services']), axis=1) # Corrected column name\n",
        "\n",
        "# 4) high_autopay_flag: PaymentMethod containing 'automatic' or 'autopay' -> often 'Electronic check' vs 'Bank transfer (automatic)'\n",
        "df['AutoPay'] = df['Payment Method'].astype(str).str.contains('automatic|auto|bank transfer|credit card', case=False, na=False).astype(int)\n",
        "\n",
        "# 5) contract_months_est: heuristically map Contract to expected months remaining (Ordinal)\n",
        "contract_map = {'Month-to-month': 1, 'One year': 12, 'Two year': 24}\n",
        "df['contract_term'] = df['Contract'].map(contract_map).fillna(1)\n",
        "\n",
        "# Additional small cleaning (drop customerID)\n",
        "if 'CustomerID' in df.columns: # Changed 'customerID' to 'CustomerID'\n",
        "    df = df.drop(columns=['CustomerID']) # Changed 'customerID' to 'CustomerID'\n",
        "\n",
        "# -----------------------\n",
        "# Feature list\n",
        "# -----------------------\n",
        "# Select features: include numeric + engineered + some categorical\n",
        "num_features = ['Tenure Months', 'Monthly Charges', 'Total Charges', 'num_services', 'avg_charge_per_service', 'contract_term'] # Corrected column names\n",
        "cat_features = ['Gender', 'Senior Citizen', 'Partner', 'Dependents', 'Phone Service',\n",
        "                'Multiple Lines', 'Internet Service', 'Online Security', 'Online Backup',\n",
        "                'Device Protection', 'Tech Support', 'Streaming TV', 'Streaming Movies',\n",
        "                'Contract', 'Paperless Billing', 'Payment Method', 'tenure_bin'] # Corrected column names\n",
        "\n",
        "# Ensure all categorical fields exist\n",
        "cat_features = [c for c in cat_features if c in df.columns]\n",
        "\n",
        "features = num_features + cat_features\n",
        "\n",
        "# Keep only selected columns + target\n",
        "df_model = df[features + [target]].copy()\n",
        "\n",
        "# Handle missing values: numeric -> median, categorical -> 'Missing'\n",
        "for c in num_features:\n",
        "    if df_model[c].isna().sum() > 0:\n",
        "        df_model[c] = df_model[c].fillna(df_model[c].median())\n",
        "for c in cat_features:\n",
        "    df_model[c] = df_model[c].fillna('Missing')\n",
        "\n",
        "# -----------------------\n",
        "# Train/test split (stratified)\n",
        "# -----------------------\n",
        "X = df_model[features]\n",
        "y = df_model[target]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, stratify=y, test_size=0.2, random_state=RANDOM_STATE\n",
        ")\n",
        "print(\"Train/test shapes:\", X_train.shape, X_test.shape)\n",
        "\n",
        "# -----------------------\n",
        "# Preprocessing pipeline\n",
        "# -----------------------\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Use OneHotEncoder for categorical (drop='rare' not available; use handle_unknown)\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, num_features),\n",
        "        ('cat', categorical_transformer, cat_features)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# Baseline model and tuning with RandomizedSearchCV\n",
        "# -----------------------\n",
        "xgb = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=4\n",
        ")\n",
        "\n",
        "pipe = Pipeline(steps=[('pre', preprocessor), ('clf', xgb)])\n",
        "\n",
        "# Quick baseline fit (to report baseline metrics before heavy tuning)\n",
        "pipe.fit(X_train, y_train)\n",
        "y_pred = pipe.predict(X_test)\n",
        "y_proba = pipe.predict_proba(X_test)[:, 1]\n",
        "baseline_auc = roc_auc_score(y_test, y_proba)\n",
        "baseline_f1 = f1_score(y_test, y_pred)\n",
        "print(f\"Baseline AUC: {baseline_auc:.4f}, Baseline F1: {baseline_f1:.4f}\")\n",
        "\n",
        "# Hyperparameter search space for XGBoost (randomized)\n",
        "param_dist = {\n",
        "    'clf__n_estimators': [100, 200, 400],\n",
        "    'clf__max_depth': [3, 4, 5, 6],\n",
        "    'clf__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'clf__subsample': [0.6, 0.8, 1.0],\n",
        "    'clf__colsample_bytree': [0.5, 0.7, 1.0],\n",
        "    'clf__reg_alpha': [0, 0.001, 0.01, 0.1],\n",
        "    'clf__reg_lambda': [1, 5, 10]\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
        "rs = RandomizedSearchCV(pipe, param_dist, n_iter=25, scoring='roc_auc', n_jobs=4, cv=cv, random_state=RANDOM_STATE, verbose=1)\n",
        "rs.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best params:\", rs.best_params_)\n",
        "best_model = rs.best_estimator_\n",
        "\n",
        "# Save model\n",
        "joblib.dump(best_model, os.path.join(OUTPUT_DIR, \"xgb_churn_model.joblib\"))\n",
        "\n",
        "# -----------------------\n",
        "# Evaluate final model\n",
        "# -----------------------\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"Final AUC: {auc:.4f}, Final F1: {f1:.4f}\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Save basic metrics to JSON\n",
        "metrics = {\"baseline_auc\": float(baseline_auc), \"baseline_f1\": float(baseline_f1),\n",
        "           \"final_auc\": float(auc), \"final_f1\": float(f1)}\n",
        "with open(os.path.join(OUTPUT_DIR, \"metrics.json\"), \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "# -----------------------\n",
        "# SHAP Global Interpretation\n",
        "# -----------------------\n",
        "# We need the trained model and the preprocessor to produce shap values.\n",
        "# Extract transformed training data to pass to SHAP\n",
        "pre = best_model.named_steps['pre']\n",
        "clf = best_model.named_steps['clf']\n",
        "\n",
        "# Build feature names after OneHotEncoding\n",
        "# get numeric names\n",
        "numeric_names = num_features\n",
        "# get categorical feature names from onehot encoder\n",
        "ohe = pre.named_transformers_['cat'].named_steps['onehot']\n",
        "ohe_feature_names = []\n",
        "if hasattr(ohe, 'get_feature_names_out'):\n",
        "    ohe_feature_names = list(ohe.get_feature_names_out(cat_features))\n",
        "else:\n",
        "    # fallback\n",
        "    for i, col in enumerate(cat_features):\n",
        "        # we cannot easily infer categories without get_feature_names_out\n",
        "        ohe_feature_names.append(col)\n",
        "\n",
        "feature_names = numeric_names + ohe_feature_names\n",
        "\n",
        "# transform a sample of training data\n",
        "X_train_trans = pre.transform(X_train)\n",
        "X_test_trans = pre.transform(X_test)\n",
        "\n",
        "# SHAP TreeExplainer for XGBoost\n",
        "explainer = shap.TreeExplainer(clf)\n",
        "# compute SHAP values (might be heavy);\n",
        "shap_vals = explainer.shap_values(X_test_trans)  # for XGBClassifier this returns array of shape (n_samples, n_features)\n",
        "# shap expects numpy arrays\n",
        "shap_summary_vals = shap_vals if isinstance(shap_vals, np.ndarray) else np.array(shap_vals)\n",
        "\n",
        "# Create a DataFrame of mean absolute SHAP values per feature\n",
        "mean_abs_shap = np.abs(shap_summary_vals).mean(axis=0)\n",
        "shap_imp_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'mean_abs_shap': mean_abs_shap\n",
        "}).sort_values('mean_abs_shap', ascending=False).reset_index(drop=True)\n",
        "\n",
        "top5_shap = shap_imp_df.head(25)  # top 25 for context, but we'll extract top5 later\n",
        "top5 = shap_imp_df.head(5)\n",
        "print(\"Top 5 features by mean(|SHAP|):\")\n",
        "print(top5)\n",
        "\n",
        "# Save SHAP summary plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "# Use SHAP's summary_plot (saves to file via plt.savefig)\n",
        "shap.summary_plot(shap_summary_vals, features=X_test_trans, feature_names=feature_names, show=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"shap_summary.png\"), dpi=150)\n",
        "plt.close()\n",
        "\n",
        "# Save shap importance to CSV\n",
        "shap_imp_df.to_csv(os.path.join(OUTPUT_DIR, \"shap_feature_importance.csv\"), index=False)\n",
        "\n",
        "# -----------------------\n",
        "# Local explanations using LIME\n",
        "# -----------------------\n",
        "# We'll pick five representative customers:\n",
        "# - 2 likely churners: top predicted probability customers from test set\n",
        "# - 2 likely non-churners: bottom predicted probability from test set\n",
        "# - 1 ambiguous: nearest to 0.5 predicted probability\n",
        "\n",
        "X_test_reset = X_test.reset_index(drop=True)\n",
        "y_test_reset = y_test.reset_index(drop=True)\n",
        "probs = best_model.predict_proba(X_test_reset)[:, 1]\n",
        "preds = best_model.predict(X_test_reset)\n",
        "\n",
        "X_test_reset['pred_proba'] = probs\n",
        "X_test_reset['pred'] = preds\n",
        "X_test_reset['true'] = y_test_reset.values\n",
        "\n",
        "# Choose indexes\n",
        "idx_top = X_test_reset.sort_values('pred_proba', ascending=False).head(2).index.tolist()\n",
        "idx_bottom = X_test_reset.sort_values('pred_proba', ascending=True).head(2).index.tolist()\n",
        "# ambiguous: closest to 0.5\n",
        "idx_amb = (X_test_reset['pred_proba'] - 0.5).abs().sort_values().head(1).index.tolist()\n",
        "selected_idx = idx_top + idx_bottom + idx_amb\n",
        "selected_idx = list(dict.fromkeys(selected_idx))  # unique\n",
        "\n",
        "print(\"Selected LIME indices:\", selected_idx)\n",
        "\n",
        "# For LIME we need the raw X_train (numpy) and feature names in original order\n",
        "# Build an array of the original features (not preprocessed) for the LimeTabularExplainer\n",
        "X_train_raw = X_train.copy()\n",
        "X_test_raw = X_test_reset.copy()\n",
        "# Note: LimeTabularExplainer wants a numpy array and class_names\n",
        "class_names = ['NoChurn', 'Churn']\n",
        "\n",
        "# We'll provide a prediction function that accepts raw rows and outputs probability for class 1\n",
        "def predict_proba_raw(X_raw):\n",
        "    # X_raw is a numpy array with columns corresponding to features\n",
        "    X_df = pd.DataFrame(X_raw, columns=X_train_raw.columns)\n",
        "    return best_model.predict_proba(X_df.values)\n",
        "\n",
        "# But LimeTabularExplainer typically expects X_train as numeric matrix.\n",
        "# We'll build an explainer using preprocessed data but also keep mapping back to original feature names.\n",
        "explainer = LimeTabularExplainer(\n",
        "    training_data=pre.transform(X_train_raw),\n",
        "    feature_names=feature_names,\n",
        "    class_names=class_names,\n",
        "    mode='classification',\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "local_explanations = {}\n",
        "\n",
        "for idx in selected_idx:\n",
        "    row_raw = X_test_raw.loc[idx:idx, X_train_raw.columns]\n",
        "    row_trans = pre.transform(row_raw)\n",
        "    exp = explainer.explain_instance(row_trans[0], clf.predict_proba, num_features=10)\n",
        "    # Save HTML explanation\n",
        "    html_path = os.path.join(OUTPUT_DIR, f\"lime_explanation_idx_{idx}.html\")\n",
        "    with open(html_path, \"w\") as f:\n",
        "        f.write(exp.as_html())\n",
        "    # Extract explanation list (feature, weight)\n",
        "    exp_list = exp.as_list()\n",
        "    local_explanations[int(idx)] = {\n",
        "        \"index\": int(idx),\n",
        "        \"pred_proba\": float(X_test_reset.loc[idx, 'pred_proba']),\n",
        "        \"pred\": int(X_test_reset.loc[idx, 'pred']),\n",
        "        \"true\": int(X_test_reset.loc[idx, 'true']),\n",
        "        \"explanation\": exp_list\n",
        "    }\n",
        "\n",
        "# Save local explanations JSON\n",
        "with open(os.path.join(OUTPUT_DIR, \"local_explanations.json\"), \"w\") as f:\n",
        "    json.dump(local_explanations, f, indent=2)\n",
        "\n",
        "# -----------------------\n",
        "# Generate plain-text reports\n",
        "# -----------------------\n",
        "# 1) Global Interpretation Summary (top 5 features)\n",
        "global_report = []\n",
        "global_report.append(\"Global Interpretation Summary (SHAP): Top features driving churn\\n\")\n",
        "for i, row in top5.iterrows():\n",
        "    fname = row['feature']\n",
        "    val = row['mean_abs_shap']\n",
        "    global_report.append(f\"{i+1}. {fname} â€” mean(|SHAP|) = {val:.5f}\")\n",
        "\n",
        "global_report_text = \"\\n\".join(global_report)\n",
        "with open(os.path.join(OUTPUT_DIR, \"global_shap_summary.txt\"), \"w\") as f:\n",
        "    f.write(global_report_text)\n",
        "\n",
        "# 2) Local Explanation Summaries (for the selected 5 customers)\n",
        "local_text_lines = []\n",
        "local_text_lines.append(\"Local Explanations (LIME) for selected customers\\n\")\n",
        "for idx, info in local_explanations.items():\n",
        "    local_text_lines.append(f\"Index {idx}: Pred_Prob={info['pred_proba']:.3f} Pred={info['pred']} True={info['true']}\")\n",
        "    for feat, weight in info['explanation']:\n",
        "        local_text_lines.append(f\"  {feat} -> weight {weight:.4f}\")\n",
        "    local_text_lines.append(\"\")\n",
        "\n",
        "local_text = \"\\n\".join(local_text_lines)\n",
        "with open(os.path.join(OUTPUT_DIR, \"local_lime_summary.txt\"), \"w\") as f:\n",
        "    f.write(local_text)\n",
        "\n",
        "# 3) Actionable Strategies (synthesis) - simple heuristic mapping\n",
        "actionable = [\n",
        "    \"1. Target month-to-month customers with high MonthlyCharges and low tenure with discounted multi-month retention offers.\",\n",
        "    \"2. For customers with high avg_charge_per_service but low number of services, offer bundle discounts and highlight value (upsell protective services).\",\n",
        "    \"3. For customers with payment friction (Electronic check) and high churn risk, encourage AutoPay adoption with small incentive and clearer billing communication.\",\n",
        "    \"4. Identify high-risk senior customers with few digital services for personalized retention calls and loyalty packages.\"\n",
        "]\n",
        "with open(os.path.join(OUTPUT_DIR, \"actionable_strategies.txt\"), \"w\") as f:\n",
        "    f.write(\"\\n\".join(actionable))\n",
        "\n",
        "# Print where outputs are saved\n",
        "print(f\"Outputs saved to folder: {OUTPUT_DIR}\")\n",
        "print(\"Files:\")\n",
        "for fn in os.listdir(OUTPUT_DIR):\n",
        "    print(\"-\", fn)\n",
        "\n",
        "# Print brief summary to console\n",
        "print(\"\\n=== Summary ===\")\n",
        "print(\"Final AUC:\", auc)\n",
        "print(\"Final F1:\", f1)\n",
        "print(\"\\nTop 5 SHAP features:\")\n",
        "print(top5.to_string(index=False))\n",
        "print(\"\\nSelected LIME indices and their pred_proba:\")\n",
        "for idx in selected_idx:\n",
        "    print(f\"- idx {idx}: prob {X_test_reset.loc[idx, 'pred_proba']:.3f}, pred {X_test_reset.loc[idx,'pred']}, true {X_test_reset.loc[idx,'true']}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ROSdsRS1t-IQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}